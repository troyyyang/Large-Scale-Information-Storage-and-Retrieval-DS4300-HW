{"paragraphs":[{"text":"// Read temperature data for 1986 and station data\n\nval temps = spark.read.csv(\"weather/1986.csv\").toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\")\nval stations = spark.read.csv(\"weather/stations.csv\").toDF(\"STATION\", \"WBAN\", \"LAT\", \"LON\")","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"temps: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\nstations: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522007553278_-1878363439","id":"20180318-191454_284618957","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:27-0400","dateFinished":"2018-03-31T19:15:28-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1768"},{"text":"// Filter bad Stations where the latitude or longitude is unavailable\n// Only include temperature data for JANUARY 28th, 1986\n// You'll want to convert latitudes and longitudes to Doubles\nval stas = stations.select(stations(\"station\"), stations(\"wban\"), stations(\"lat\").cast(\"double\"), stations(\"lon\").cast(\"double\")).filter($\"LAT\".isNotNull && $\"LON\".isNotNull)\nval tmps = temps.filter($\"month\"=== 1 && $\"day\" === 28)\n","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stas: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station: string, wban: string ... 2 more fields]\ntmps: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [STATION: string, WBAN: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522007553280_-1794488179","id":"20180318-191924_1310538952","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:27-0400","dateFinished":"2018-03-31T19:15:28-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1769"},{"text":"// Define a function that compute the distance between two points on the Earth using the Haversine formula\n// https://www.movable-type.co.uk/scripts/latlong.html\n// Declare the function as a UDF (User-defined function) so that it can be applied\n\nval pi = 3.14159265\nval REarth = 6371.0 // kilometers\ndef toRadians(x: Double): Double = x * pi / 180.0\n\ndef haversine(lat1: Double, lon1: Double, lat2: Double, lon2: Double): Double = {\n    val dlon = toRadians(lon2-lon1) \n    val dlat = toRadians(lat2-lat1)\n    \n    val a = math.pow(math.sin(dlat/2), 2) + math.cos(toRadians(lat1)) * math.cos(toRadians(lat2)) * math.pow(math.sin(dlon/2), 2)\n    val c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n    REarth * c\n    \n}\n\n// Now you can use \"haver\" as a function with Spark SQL \nimport org.apache.spark.sql.functions.udf\nval haver = udf(haversine _)","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"pi: Double = 3.14159265\nREarth: Double = 6371.0\ntoRadians: (x: Double)Double\nhaversine: (lat1: Double, lon1: Double, lat2: Double, lon2: Double)Double\nimport org.apache.spark.sql.functions.udf\nhaver: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function4>,DoubleType,Some(List(DoubleType, DoubleType, DoubleType, DoubleType)))\n"}]},"apps":[],"jobName":"paragraph_1522007553280_-1794488179","id":"20180318-195342_1996330025","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:28-0400","dateFinished":"2018-03-31T19:15:29-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1770"},{"text":"// Find all stations within 100 km using your haversine function\n\nval capeCanaveralLatitude = 28.388382\nval capeCanaveralLongitude = -80.603498\n\nval st100 = stas.filter(haver($\"lat\", $\"lon\", lit(capeCanaveralLatitude), lit(capeCanaveralLongitude)) <= 100)\n","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"capeCanaveralLatitude: Double = 28.388382\ncapeCanaveralLongitude: Double = -80.603498\nst100: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [station: string, wban: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522007553281_-1794872927","id":"20180318-191955_1910278437","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:29-0400","dateFinished":"2018-03-31T19:15:29-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1771"},{"text":"// Use inverse distance weighting to estimate the temperature at Cape Canaveral on that day\n// You might do this in serveral steps.  First compute a weight for each station within 100 km that recorded\n// a temperature.  Then, when you have a column of weights, apply an aggregation function to\n// multiply each station temperature by a weight and to compute the sum of the weights.\n// This link explains more on inverse distance weighting:\n// https://en.wikipedia.org/wiki/Inverse_distance_weighting\n// Use p=2 in the formula\nimport org.apache.spark.sql.functions.pow\n\n\nval stidw = st100.withColumn(\"idw\", pow(haver($\"lat\", $\"lon\", lit(capeCanaveralLatitude), lit(capeCanaveralLongitude)), -2))\n\n","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.pow\nstidw: org.apache.spark.sql.DataFrame = [station: string, wban: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1522007553281_-1794872927","id":"20180319-091159_1767904139","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:29-0400","dateFinished":"2018-03-31T19:15:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1772"},{"text":"// Once you have the weighted sum of temperatures (numerator) and the sum of the weights (denominator)\n// you can obtain your final result\n\nval stnonull = stidw.na.fill(Map(\"wban\" -> 0, \"station\" -> 0))\nval tmpnonull = tmps.na.fill(Map(\"wban\" -> 0, \"station\" -> 0))\n\nval complete = stnonull.join(tmpnonull, stnonull(\"wban\") === tmpnonull(\"wban\") && stnonull(\"station\") === tmpnonull(\"station\"))\n\ncomplete.agg(sum($\"idw\" * $\"temp\")/sum($\"idw\")).head.getDouble(0)\n\n\n","user":"anonymous","dateUpdated":"2018-03-31T19:15:27-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"stnonull: org.apache.spark.sql.DataFrame = [station: string, wban: string ... 3 more fields]\ntmpnonull: org.apache.spark.sql.DataFrame = [STATION: string, WBAN: string ... 3 more fields]\ncomplete: org.apache.spark.sql.DataFrame = [station: string, wban: string ... 8 more fields]\nres409: Double = 36.44586052803954\n"}]},"apps":[],"jobName":"paragraph_1522007553281_-1794872927","id":"20180318-194257_301533474","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:29-0400","dateFinished":"2018-03-31T19:15:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1773"},{"text":"// Extra credit - find average temperature for Jan 28 for every year.\n// Generate a line plot.\n// Was the Jan 28, 1986 temperature unusual?\n\nimport scala.collection.mutable.ListBuffer\n\n\nvar jan28temps = new ListBuffer[Double]()\n\n\nfor ( i <- 1975 to 2015) {\n    val file = \"weather/\" + i.toString + \".csv\"\n    val yeartemp = spark.read.csv(file).toDF(\"STATION\", \"WBAN\", \"MONTH\", \"DAY\", \"TEMP\").filter($\"month\"=== 1 && $\"day\" === 28).na.fill(Map(\"wban\" -> 0, \"station\" -> 0))\n    val complete = stnonull.join(yeartemp, stnonull(\"wban\") === yeartemp(\"wban\") && stnonull(\"station\") === yeartemp(\"station\"))\n    jan28temps += complete.agg(sum($\"idw\" * $\"temp\")/sum($\"idw\")).head.getDouble(0)\n}\n\n\nprint(jan28temps.toList)\n\n// it appears that it was unusually cold on jan 28 1986\n// unfortunatley I couldn't figure out how to install libraries/manage dependencies with scala, so I couldn't create a graph\n\n\n\n\n\n","user":"anonymous","dateUpdated":"2018-03-31T19:18:49-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scala.collection.mutable.ListBuffer\njan28temps: scala.collection.mutable.ListBuffer[Double] = ListBuffer()\nList(68.16990400144356, 56.60664651250081, 64.27939016270864, 46.81876634279648, 58.58724177696329, 63.486221441197586, 60.45326348054837, 59.617027967703024, 55.346649054042146, 63.17216582309038, 61.1737131329579, 36.44586052803954, 50.17599111555688, 46.73752323622757, 66.79619782251716, 67.70461106403616, 71.81746409831388, 66.73857800444078, 56.87851414729612, 73.49341268005949, 64.67947863188655, 65.17467660670118, 69.36539374848826, 58.41567691512577, 69.31223179119857, 54.544476057136336, 59.25148600393138, 71.87680597422217, 55.622066433832615, 53.8889208166924, 62.77118146794948, 65.94678902151495, 65.53583809616468, 55.66126861558825, 69.24100365898292, 56.46877896667552, 54.582015934245334, 62.70143756726345, 71.04600842987263, 66.37576853834587, 54.53986392477423)"}]},"apps":[],"jobName":"paragraph_1522007553282_-1793718681","id":"20180318-194403_1746372836","dateCreated":"2018-03-25T15:52:33-0400","dateStarted":"2018-03-31T19:15:30-0400","dateFinished":"2018-03-31T19:16:12-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1774"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1522519610200_-1458310568","id":"20180331-140650_37746461","dateCreated":"2018-03-31T14:06:50-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1775","text":"//import co.theasi.plotly\n\n//val xs = (1975 until 2015)\n\n//val p = Plot().withScatter(xs, jan28temps.toList)\n\n//draw(p, \"basic-scatter\", writer.FileOptions(overwrite=true))","dateUpdated":"2018-03-31T19:15:27-0400","dateFinished":"2018-03-31T19:16:12-0400","dateStarted":"2018-03-31T19:15:32-0400","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1522537488750_1567193924","id":"20180331-190448_108213799","dateCreated":"2018-03-31T19:04:48-0400","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3841"}],"name":"yang_hw5","id":"2D919BRAY","angularObjects":{"2D98QSA92:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}